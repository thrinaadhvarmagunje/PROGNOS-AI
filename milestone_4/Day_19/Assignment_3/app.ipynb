{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMf63L0pnJwADf5slCYBvMn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TA2J9Ct-08K","executionInfo":{"status":"ok","timestamp":1759124394857,"user_tz":-330,"elapsed":21713,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}},"outputId":"57e4bd27-f5f4-4c5e-f267-e5f3cb34178a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Unr--V01e-X","executionInfo":{"status":"ok","timestamp":1759121963174,"user_tz":-330,"elapsed":12530,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}},"outputId":"62d6e7a4-9340-4f22-c926-22f353845fe2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting pyngrok\n","  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n","Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n","Successfully installed pydeck-0.9.1 pyngrok-7.4.0 streamlit-1.50.0\n"]}],"source":["!pip install streamlit pyngrok\n"]},{"cell_type":"code","source":["# app.py\n","%%writefile app.py\n","\n","# app.py\n","\"\"\"\n","Streamlit app (updated loader) for loading a saved Iris model and:\n","- Prediction UI (inputs -> prediction + probabilities)\n","- Data exploration (histograms, scatter plots)\n","- Robust model-loading from dict/tuple / GridSearchCV etc.\n","\"\"\"\n","\n","import streamlit as st\n","import pandas as pd\n","import numpy as np\n","from joblib import load\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","import types\n","import traceback\n","\n","st.set_page_config(page_title=\"Iris Classifier: Predict & Explore\", layout=\"wide\")\n","\n","# ---------------------------\n","# Robust model extraction\n","# ---------------------------\n","def _recursive_find_model(obj):\n","    \"\"\"Search an object (possibly nested dict/list) for a sklearn-like estimator (has .predict).\n","       Returns tuple (model_obj, class_names, path_str) or (None, None, None).\n","    \"\"\"\n","    visited = set()\n","    def _search(o, path=\"root\"):\n","        oid = id(o)\n","        if oid in visited:\n","            return None\n","        visited.add(oid)\n","\n","        # direct estimator\n","        if hasattr(o, \"predict\") and callable(getattr(o, \"predict\")):\n","            return o, None, path\n","\n","        # GridSearchCV / RandomizedSearchCV-like wrapper\n","        if hasattr(o, \"best_estimator_\") and hasattr(o.best_estimator_, \"predict\"):\n","            return o.best_estimator_, None, path + \"['best_estimator_']\"\n","\n","        # If it's a dict, look for common keys first and then search nested values\n","        if isinstance(o, dict):\n","            # quick-check for common keys\n","            for key in (\"model\", \"estimator\", \"pipeline\", \"clf\", \"classifier\", \"trained_model\", \"best_estimator\", \"est\"):\n","                if key in o:\n","                    candidate = o[key]\n","                    if hasattr(candidate, \"predict\"):\n","                        # try to find class names in same dict\n","                        class_names = None\n","                        for cn_key in (\"class_names\", \"target_names\", \"labels\", \"classes\"):\n","                            if cn_key in o:\n","                                class_names = o[cn_key]\n","                                break\n","                        return candidate, class_names, path + f\"['{key}']\"\n","            # otherwise deep search values\n","            for k, v in o.items():\n","                res = _search(v, path + f\"['{k}']\")\n","                if res:\n","                    return res\n","\n","        # list / tuple\n","        if isinstance(o, (list, tuple)):\n","            for idx, item in enumerate(o):\n","                res = _search(item, path + f\"[{idx}]\")\n","                if res:\n","                    return res\n","\n","        # object attributes (instances)\n","        if hasattr(o, \"__dict__\"):\n","            for k, v in vars(o).items():\n","                res = _search(v, path + f\".{k}\")\n","                if res:\n","                    return res\n","\n","        return None\n","\n","    result = _search(obj)\n","    if result:\n","        # result is (model_obj, class_names, path)\n","        return result\n","    return None, None, None\n","\n","@st.cache_resource\n","def load_model(path: str):\n","    \"\"\"Load file and attempt to extract a model and class names.\"\"\"\n","    try:\n","        raw = load(path)\n","    except Exception as e:\n","        return None, None, {\"error\": f\"joblib.load failed: {e}\", \"trace\": traceback.format_exc()}\n","\n","    # If raw itself is a model-like object\n","    if hasattr(raw, \"predict\") and callable(getattr(raw, \"predict\")):\n","        return raw, None, {\"type\": type(raw).__name__, \"detail\": \"top-level estimator\"}\n","\n","    # If it's a GridSearchCV-like object saved directly\n","    if hasattr(raw, \"best_estimator_\") and hasattr(raw.best_estimator_, \"predict\"):\n","        return raw.best_estimator_, None, {\"type\": type(raw).__name__, \"detail\": \"best_estimator_ extracted\"}\n","\n","    # If it's a dict/list/tuple, try to find inner estimator\n","    model_obj, class_names, path = _recursive_find_model(raw)\n","    if model_obj is not None:\n","        meta = {\"type\": type(model_obj).__name__, \"found_at\": path}\n","        # try to also expose raw keys if top-level dict\n","        if isinstance(raw, dict):\n","            meta[\"raw_keys\"] = list(raw.keys())\n","        return model_obj, class_names, meta\n","\n","    # fallback: no model found\n","    meta = {\"type\": type(raw).__name__, \"raw_preview\": None}\n","    if isinstance(raw, dict):\n","        meta[\"raw_keys\"] = list(raw.keys())\n","    if isinstance(raw, (list, tuple)):\n","        meta[\"len\"] = len(raw)\n","    # small preview\n","    try:\n","        meta[\"raw_preview\"] = str(raw)[:500]\n","    except Exception:\n","        meta[\"raw_preview\"] = \"<unable to preview raw object>\"\n","    return None, None, meta\n","\n","# ---------------------------\n","# Other helpers\n","# ---------------------------\n","@st.cache_resource\n","def load_sample_data():\n","    iris = datasets.load_iris(as_frame=True)\n","    df = iris.frame.copy()\n","    df = df.rename(columns={\n","        'sepal length (cm)': 'sepal_length',\n","        'sepal width (cm)': 'sepal_width',\n","        'petal length (cm)': 'petal_length',\n","        'petal width (cm)': 'petal_width',\n","        'target': 'target'\n","    })\n","    target_names = iris.target_names.tolist()\n","    df['target_name'] = df['target'].map(lambda t: target_names[int(t)])\n","    return df, list(df.columns[:4]), target_names\n","\n","def predict_and_format(model, X: np.ndarray, provided_class_names=None):\n","    if model is None:\n","        return None, (None, None)\n","    preds = model.predict(X)\n","    probs = None\n","    try:\n","        probs = model.predict_proba(X)\n","    except Exception:\n","        probs = None\n","\n","    # Determine class labels order for probabilities\n","    class_labels = None\n","    if provided_class_names:\n","        class_labels = list(provided_class_names)\n","    else:\n","        try:\n","            if hasattr(model, \"classes_\"):\n","                class_labels = list(model.classes_)\n","        except Exception:\n","            class_labels = None\n","\n","    return preds, (probs, class_labels)\n","\n","# ---------------------------\n","# App layout & behavior\n","# ---------------------------\n","st.title(\"Iris Species Classifier — Predict & Explore (robust loader)\")\n","st.markdown(\"This app attempts to find a species of flower based on its measurements.\")\n","\n","MODEL_PATH = \"/content/drive/MyDrive/Major_Project_1/iris_model.joblib\"\n","model, saved_class_names, model_meta = load_model(MODEL_PATH)\n","sample_df, feature_names, default_target_names = load_sample_data()\n","target_names = saved_class_names if saved_class_names is not None else default_target_names\n","\n","# Sidebar\n","with st.sidebar:\n","    st.header(\"Controls\")\n","    mode = st.radio(\"Mode\", [\"Prediction\", \"Data Exploration\"], index=0)\n","    st.markdown(\"---\")\n","    st.write(\"Model status:\")\n","    if model is None:\n","        st.error(\"No usable model found.\")\n","        # show helpful debugging info\n","        st.json(model_meta)\n","        st.info(\"If you saved a dict like {'model': estimator, 'class_names': [...]}, this loader will find it. If not found, please share the joblib keys or re-save your estimator as the top-level object.\")\n","    else:\n","        st.success(\"Model loaded.\")\n","        st.caption(f\"Extracted model type: `{model_meta.get('type','?')}`\")\n","        if \"found_at\" in model_meta:\n","            st.caption(f\"Found at: {model_meta['found_at']}\")\n","        if model_meta.get(\"raw_keys\"):\n","            st.write(\"Top-level keys:\", \", \".join(map(str, model_meta[\"raw_keys\"])))\n","    st.markdown(\"---\")\n","    st.write(f\"Sample dataset: {sample_df.shape[0]} rows • features: {', '.join(feature_names)}\")\n","\n","# Prediction Mode\n","if mode == \"Prediction\":\n","    st.subheader(\"Prediction Mode\")\n","    st.markdown(\"Enter measurements and click **Predict**.\")\n","\n","    left_col, right_col = st.columns([1, 1])\n","    with left_col:\n","        def slider_for_feature(df, feat):\n","            fmin = float(df[feat].min())\n","            fmax = float(df[feat].max())\n","            frange = fmax - fmin\n","            return st.slider(\n","                label=feat.replace(\"_\", \" \").title(),\n","                min_value=round(fmin - 0.2 * frange, 2),\n","                max_value=round(fmax + 0.2 * frange, 2),\n","                value=float(df[feat].median()),\n","                step=0.01,\n","                help=f\"Measured {feat.replace('_',' ')} in cm (approx: {fmin:.2f}–{fmax:.2f})\"\n","            )\n","\n","        sepal_length = slider_for_feature(sample_df, \"sepal_length\")\n","        sepal_width  = slider_for_feature(sample_df, \"sepal_width\")\n","        petal_length = slider_for_feature(sample_df, \"petal_length\")\n","        petal_width  = slider_for_feature(sample_df, \"petal_width\")\n","\n","        st.markdown(\"#### Batch input (optional)\")\n","        batch_text = st.text_area(\n","            \"Paste CSV rows (no header) to predict multiple at once (order: sepal_length,sepal_width,petal_length,petal_width)\",\n","            value=\"\",\n","            height=80\n","        )\n","        predict_button = st.button(\"Predict\")\n","\n","    with right_col:\n","        st.markdown(\"### Prediction result\")\n","        if predict_button:\n","            # parse input\n","            try:\n","                if batch_text.strip():\n","                    rows = [r.strip() for r in batch_text.strip().splitlines() if r.strip()]\n","                    parsed = []\n","                    for r in rows:\n","                        parts = [p.strip() for p in r.split(\",\")]\n","                        if len(parts) != 4:\n","                            raise ValueError(\"Each row must have 4 comma-separated values.\")\n","                        parsed.append([float(x) for x in parts])\n","                    X = np.array(parsed)\n","                else:\n","                    X = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n","            except Exception as e:\n","                st.error(f\"Input parsing error: {e}\")\n","                X = None\n","\n","            if X is not None:\n","                if model is None:\n","                    st.error(\"Model not available — cannot predict. See sidebar for diagnostics.\")\n","                else:\n","                    try:\n","                        preds, (probs, class_labels) = predict_and_format(model, X, provided_class_names=saved_class_names)\n","                        for i, single_pred in enumerate(preds):\n","                            # determine display name\n","                            pred_name = None\n","                            try:\n","                                # if model returns numeric labels and default target names exist\n","                                if isinstance(single_pred, (int, np.integer)):\n","                                    pred_name = target_names[int(single_pred)]\n","                                else:\n","                                    pred_name = str(single_pred)\n","                            except Exception:\n","                                pred_name = str(single_pred)\n","\n","                            # simple color coding\n","                            color_map = {\n","                                target_names[0]: \"#138000\",\n","                                target_names[1]: \"#e67700\",\n","                                target_names[2]: \"#c50000\",\n","                            }\n","                            display_color = color_map.get(pred_name, \"#333333\")\n","                            st.markdown(f\"**Sample {i+1} prediction:**\")\n","                            st.markdown(f\"<div style='padding:10px;border-radius:8px;background:{display_color};color:white;'><strong style='font-size:18px'>{pred_name}</strong></div>\", unsafe_allow_html=True)\n","\n","                            if probs is not None and class_labels is not None:\n","                                prob_series = pd.Series(probs[i], index=class_labels)\n","                                st.markdown(\"Prediction probabilities:\")\n","                                st.bar_chart(prob_series)\n","                            elif probs is not None:\n","                                # if we don't know class labels, display numeric columns\n","                                prob_df = pd.DataFrame(probs, columns=[f\"c{i}\" for i in range(probs.shape[1])])\n","                                st.markdown(\"Prediction probabilities (no class labels available):\")\n","                                st.table(prob_df.iloc[i])\n","                    except Exception as e:\n","                        st.error(f\"Prediction error: {e}\")\n","                        st.exception(traceback.format_exc())\n","        else:\n","            st.info(\"Set inputs and click Predict.\")\n","\n","    st.markdown(\"---\")\n","    if st.checkbox(\"Show current input as table\"):\n","        input_df = pd.DataFrame([{\n","            \"sepal_length\": sepal_length,\n","            \"sepal_width\": sepal_width,\n","            \"petal_length\": petal_length,\n","            \"petal_width\": petal_width\n","        }])\n","        st.table(input_df)\n","\n","# Data Exploration Mode\n","else:\n","    st.subheader(\"Data Exploration Mode\")\n","    st.markdown(\"Explore the sample Iris dataset.\")\n","\n","    exp_col1, exp_col2 = st.columns([1, 2])\n","    with exp_col1:\n","        st.markdown(\"### Histogram\")\n","        hist_feature = st.selectbox(\"Choose feature for histogram\", options=feature_names, index=0)\n","        bins = st.slider(\"Bins\", min_value=5, max_value=50, value=15)\n","\n","        st.markdown(\"### Scatter plot\")\n","        x_feat = st.selectbox(\"X axis\", options=feature_names, index=0)\n","        y_feat = st.selectbox(\"Y axis\", options=feature_names, index=2)\n","        hue_by_target = st.checkbox(\"Color by species\", value=True)\n","\n","    with exp_col2:\n","        fig_hist, ax_hist = plt.subplots()\n","        ax_hist.hist(sample_df[hist_feature], bins=bins)\n","        ax_hist.set_xlabel(hist_feature.replace(\"_\", \" \").title())\n","        ax_hist.set_ylabel(\"Count\")\n","        ax_hist.set_title(f\"Histogram of {hist_feature.replace('_',' ').title()}\")\n","        st.pyplot(fig_hist)\n","\n","        fig_scat, ax_scat = plt.subplots()\n","        if hue_by_target:\n","            for t_name in sample_df['target_name'].unique():\n","                sub = sample_df[sample_df['target_name'] == t_name]\n","                ax_scat.scatter(sub[x_feat], sub[y_feat], label=t_name, alpha=0.8, edgecolors='w', s=60)\n","            ax_scat.legend(title=\"Species\")\n","        else:\n","            ax_scat.scatter(sample_df[x_feat], sample_df[y_feat], alpha=0.8)\n","        ax_scat.set_xlabel(x_feat.replace(\"_\",\" \").title())\n","        ax_scat.set_ylabel(y_feat.replace(\"_\",\" \").title())\n","        ax_scat.set_title(f\"{y_feat.replace('_',' ').title()} vs {x_feat.replace('_',' ').title()}\")\n","        st.pyplot(fig_scat)\n","\n","    st.markdown(\"---\")\n","    if st.checkbox(\"Show raw dataset table\"):\n","        st.dataframe(sample_df)\n","\n","# Footer\n","st.markdown(\"---\")\n","st.markdown(\n","    \"\"\"\n","    **Notes**\n","    - THIS MODEL WILL PREDICT DIFFERENT SPECIES OF FLOWERS   \"\"\"\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ojtKpG5CrmB","executionInfo":{"status":"ok","timestamp":1759126389254,"user_tz":-330,"elapsed":87,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}},"outputId":"e5403f85-3ad5-498c-934c-25c8f82800e4"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["!ngrok config add-authtoken 30ACubmdi0jxHkaCPKpc9Dg9um8_6tPcJmskLiruGphPRmX8v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpP8714I4oZk","executionInfo":{"status":"ok","timestamp":1759126231558,"user_tz":-330,"elapsed":219,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}},"outputId":"77cda0d2-83a5-49b8-8395-d3589342578d"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":["import os\n","import threading\n","def run_streamlit():\n","  os.system('streamlit run app.py --server.port 8501')\n","thread=threading.Thread(target=run_streamlit)\n","thread.start()"],"metadata":{"id":"HbmhIwVf1zgl","executionInfo":{"status":"ok","timestamp":1759126232547,"user_tz":-330,"elapsed":21,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import time\n","\n","# Disconnect any existing tunnels to avoid the limit\n","for tunnel in ngrok.get_tunnels():\n","    ngrok.disconnect(tunnel.public_url)\n","\n","time.sleep(5)\n","public_url = ngrok.connect(8501)\n","print(\"your streamlit app is live here:\", public_url)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPZtdeY619Zd","executionInfo":{"status":"ok","timestamp":1759126239316,"user_tz":-330,"elapsed":5159,"user":{"displayName":"THRINAADHVARMA GUNJE","userId":"00603355172490099165"}},"outputId":"f065b594-486e-4770-f234-fb88e5aa0af2"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-09-29T06:10:33+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-45c26ac4-069e-4575-a265-1defdde9e924 acceptErr=\"failed to accept connection: Listener closed\"\n"]},{"output_type":"stream","name":"stdout","text":["your streamlit app is live here: NgrokTunnel: \"https://32ba8b3af5d6.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OP8glYoU30Yo"},"execution_count":null,"outputs":[]}]}